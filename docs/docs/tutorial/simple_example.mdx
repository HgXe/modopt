---
sidebar_position: 1
---

# A Simple example to get started

## Define your problem

Let's start with a simple problem of minimizing $x_1^2 + x_2^2$ with respect to $x_1$ and $x_2$.

The numerical problem statement is : 


$$
\underset{x_1, x_2 \in \mathbb{R}}{\text{minimize}} \quad x_1^2 + x_2^2
$$

We know the solution for this problem is $x_1=0$ and $x_2=0$.
However, we start from an intial guess of $x_1=5$ and $x_2=10$ for the purposes of this tutorial.

The problem is written in modOpt using the **Problem()** class as follows:


```py
import numpy as np

from modopt.api import Problem

class X2(Problem):
    def initialize(self, ):
        # Name your problem
        self.problem_name = 'x^4'

    def setup(self):
        # Add design variables of your problem
        self.add_design_variables('x',
                                  shape=(2, ),
                                  vals=np.array([5., 10.])
                                  )

        # # Add the objective your problem
        # self.add_objective('obj')

    def setup_derivatives(self):
        # Declare objective gradient and it's shape
        self.declare_objective_gradient(wrt='x',
                                        shape=(2, ),
                                        )

    # Compute the value of the objective with given design variable values
    def compute_objective(self, x):
        return x[0] ** 2 + x[1] ** 2

    def compute_objective_gradient(self, x):
        return 2 * x

```

## Develop/Build your optimization algorithm

Here we look at the **steepest descent** algorithm for unconstrained problems. 
We will later (in the next section) use it to solve the unconstrained  optimization problem defined above.

For a general unconstrained optimization problem stated as: 

$$
\underset{x \in \mathbb{R^n}}{\text{minimize}} \quad f(x)
$$

the steepest descent algorithms computes the new iterate recursively by using the formula

$$
x_{k+1} = x_{k} - \nabla f(x_k) .
$$

Given an initial guess $x_0$, we can write an optimizer using the steepest descent algorithm using the **Optimizer()** class in modOpt as follows:

```py
import numpy as np
import time

from modopt.api import Optimizer


class SteepestDescent(Optimizer):

    def initialize(self):
        # Name your algorithm
        self.solver = 'steepest_descent'

    def run(self):
        nx = self.nx
        x0 = x0 = self.problem.x.get_data()
        opt_tol = self.options['opt_tol']
        max_itr = self.options['max_itr']

        obj = self.obj
        grad = self.grad

        start_time = time.time()

        # Setting intial values for current iterates
        x_k = x0 * 1.
        f_k = obj(x_k)
        g_k = grad(x_k)
        
        # Setting intial values for computed new iterates
        f_new = f_k * 1
        g_new = g_k * 1
        
        itr = 0

        # Initializing result arrays
        num_f_evals_array = np.array([1])
        num_g_evals_array = np.array([1])
        x_array = x0.reshape(1, nx)
        obj_array = np.array([f_k * 1.])
        opt_array = np.array([np.linalg.norm(g_k)])
        step_array = np.array([0.,])
        time_array = np.array([time.time() - start_time])
        
        while (opt_array[-1] > opt_tol and itr < max_itr):
            start = time.time()
            itr += 1

            p_k =  -g_k   
        
            x_k += p_k
            f_k = obj(x_k)
            g_k = grad(x_k)

            x_array = np.append(x_array, x_k.reshape(1, nx), axis=0)
            obj_array = np.append(obj_array, f_k)
            opt_array = np.append(opt_array, np.linalg.norm(g_k))
            end = time.time()
            time_array = np.append(time_array, [time_array[-1] + end - start])

        end_time = time.time()
        itr_array = np.arange(itr + 1)
        self.total_time = end_time - start_time

        self.results['x_array'] = x_array
        self.results['itr_array'] = itr_array
        self.results['obj_array'] = obj_array
        self.results['opt_array'] = opt_array
        self.results['time_array'] = time_array


```

The **Optimizer()** class records all the data needed using the `results` dictionary.

## Solve your problem using your optimizer

Now that we have modeled the problem and developed the optimizer, the task remaining is to solve the problem with the optimizer.
For this, we need to set up our optimizer with the problem and pass in optimizer-specific parameters. 
Default values will be assumed if the optimizer parameters are not passed in.

```py

# Set your optimality tolerance
opt_tol = 1E-8
# Set maximum optimizer iteration limit
max_itr = 500

prob = X2()

# Set up your optimizer with your problem and pass in optimizer parameters
optimizer = SteepestDescent(prob, opt_tol=opt_tol, max_itr=max_itr)

# Check first derivatives at the initial guess, if needed
optimizer.check_first_derivatives(prob.x.get_data())

# Solve your optimization problem
optimizer.solve()

# Print results of optimization (summary_table contains information from each iteration)
optimizer.print_results(summary_table=True)

```




