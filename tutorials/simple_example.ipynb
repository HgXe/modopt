{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple example (unconstrained)\n",
    "\n",
    "## Define your problem\n",
    "\n",
    "Let's start with a simple problem of minimizing $x_1^4 + x_2^4$ with respect to $x_1$ and $x_2$.\n",
    "\n",
    "The mathematical problem statement is: \n",
    "\n",
    "\n",
    "$$\n",
    "\\underset{x_1, x_2 \\in \\mathbb{R}}{\\text{minimize}} \\quad x_1^4 + x_2^4\n",
    "$$\n",
    "\n",
    "We know the solution of this problem is $x_1=0$, and $x_2=0$.\n",
    "However, we start from an intial guess of $x_1=0.3$, and $x_2=0.3$ for the purposes of this tutorial.\n",
    "\n",
    "The problem is written in modOpt using the **Problem()** class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anugrah/Packages/mdo_wi2024/modopt/modopt/external_libraries/snopt/snoptc.py:6: UserWarning: snoptc from 'optimize' could not be imported\n",
      "  warnings.warn(\"snoptc from 'optimize' could not be imported\")\n",
      "/Users/anugrah/Packages/mdo_wi2024/modopt/modopt/external_libraries/snopt/snoptc.py:10: UserWarning: snopt7_python from 'optimize.solvers' could not be imported\n",
      "  warnings.warn(\"snopt7_python from 'optimize.solvers' could not be imported\")\n",
      "/Users/anugrah/Packages/mdo_wi2024/modopt/modopt/external_libraries/snopt/snopt_optimizer.py:6: UserWarning: SNOPT_options from 'optimize' could not be imported\n",
      "  warnings.warn(\"SNOPT_options from 'optimize' could not be imported\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from modopt import Problem\n",
    "\n",
    "\n",
    "class X4(Problem):\n",
    "    def initialize(self, ):\n",
    "        # Name your problem\n",
    "        self.problem_name = 'x^4'\n",
    "\n",
    "    def setup(self):\n",
    "        # Add design variables of your problem\n",
    "        self.add_design_variables('x',\n",
    "                                  shape=(2, ),\n",
    "                                  vals=np.array([.3, .3]))\n",
    "        self.add_objective('f')\n",
    "\n",
    "    def setup_derivatives(self):\n",
    "        # Declare objective gradient and its shape\n",
    "        self.declare_objective_gradient(wrt='x', )\n",
    "\n",
    "    # Compute the value of the objective with given design variable values\n",
    "    def compute_objective(self, dvs, obj):\n",
    "        obj['f'] = np.sum(dvs['x']**4)\n",
    "\n",
    "    def compute_objective_gradient(self, dvs, grad):\n",
    "        grad['x'] = 4 * dvs['x']**3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop/Build your optimization algorithm\n",
    "\n",
    "Here we look at the **steepest descent** algorithm for unconstrained problems. \n",
    "We will later (in the next section) use it to solve the unconstrained  optimization problem defined above.\n",
    "\n",
    "For a general unconstrained optimization problem stated as: \n",
    "\n",
    "$$\n",
    "\\underset{x \\in \\mathbb{R^n}}{\\text{minimize}} \\quad f(x)\n",
    "$$\n",
    "\n",
    "the steepest descent algorithms computes the new iterate recursively by using the formula\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_{k} - \\nabla f(x_k) .\n",
    "$$\n",
    "\n",
    "Given an initial guess $x_0$, we can write an optimizer using the steepest descent algorithm using the **Optimizer()** class in modOpt as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from modopt import Optimizer\n",
    "\n",
    "\n",
    "class SteepestDescent(Optimizer):\n",
    "    def initialize(self):\n",
    "\n",
    "        # Name your algorithm\n",
    "        self.solver_name = 'steepest_descent'\n",
    "\n",
    "        self.obj = self.problem._compute_objective\n",
    "        self.grad = self.problem._compute_objective_gradient\n",
    "\n",
    "        self.options.declare('maxiter', default=1000, types=int)\n",
    "        self.options.declare('opt_tol', default=1e-5, types=float)\n",
    "\n",
    "        # Specify format of outputs available from your optimizer after each iteration\n",
    "        self.available_outputs = {\n",
    "            'itr': int,\n",
    "            'obj': float,\n",
    "            # for arrays from each iteration, shapes need to be declared\n",
    "            'x': (float, (self.problem.nx, )),\n",
    "            'opt': float,\n",
    "            'time': float,\n",
    "        }\n",
    "\n",
    "        # Enable user to specify, as a list, which among the available outputs\n",
    "        # need to be stored in memory and written to output files\n",
    "        self.options.declare('outputs',\n",
    "                             types=list,\n",
    "                             default=['itr', 'obj', 'x', 'opt', 'time'])\n",
    "\n",
    "    def solve(self):\n",
    "        nx = self.problem.nx\n",
    "        x = self.problem.x0\n",
    "        opt_tol = self.options['opt_tol']\n",
    "        maxiter = self.options['maxiter']\n",
    "\n",
    "        obj = self.obj\n",
    "        grad = self.grad\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Setting intial values for initial iterates\n",
    "        x_k = x * 1.\n",
    "        f_k = obj(x_k)\n",
    "        g_k = grad(x_k)\n",
    "\n",
    "        # Iteration counter\n",
    "        itr = 0\n",
    "\n",
    "        # Optimality\n",
    "        opt = np.linalg.norm(g_k)\n",
    "\n",
    "        # Initializing outputs\n",
    "        self.update_outputs(itr=0,\n",
    "                            x=x_k,\n",
    "                            obj=f_k,\n",
    "                            opt=opt,\n",
    "                            time=time.time() - start_time)\n",
    "\n",
    "        while (opt > opt_tol and itr < maxiter):\n",
    "            itr_start = time.time()\n",
    "            itr += 1\n",
    "\n",
    "            # ALGORITHM STARTS HERE\n",
    "            # >>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "            p_k = -g_k\n",
    "\n",
    "            x_k += p_k\n",
    "            f_k = obj(x_k)\n",
    "            g_k = grad(x_k)\n",
    "\n",
    "            opt = np.linalg.norm(g_k)\n",
    "\n",
    "            # <<<<<<<<<<<<<<<<<<<\n",
    "            # ALGORITHM ENDS HERE\n",
    "\n",
    "            # Append arrays inside outputs dict with new values from the current iteration\n",
    "            self.update_outputs(itr=itr,\n",
    "                                x=x_k,\n",
    "                                obj=f_k,\n",
    "                                opt=opt,\n",
    "                                time=time.time() - start_time)\n",
    "\n",
    "        # Run post-processing for the Optimizer() base class\n",
    "        self.run_post_processing()\n",
    "\n",
    "        self.total_time = time.time() - start_time\n",
    "\n",
    "        self.results = {\n",
    "            'x': x_k,\n",
    "            'obj': f_k,\n",
    "            'opt': opt,\n",
    "            'itr': itr,\n",
    "            'time': self.total_time\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Optimizer()** class records all the data needed using the `outputs` dictionary.\n",
    "\n",
    "## Solve your problem using your optimizer\n",
    "\n",
    "Now that we have modeled the problem and developed the optimizer, the task remaining is to solve the problem with the optimizer.\n",
    "For this, we need to set up our optimizer with the problem and pass in optimizer-specific parameters. \n",
    "Default values will be assumed if the optimizer parameters are not passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting objective name as \"f\".\n",
      "Directory  x^4_outputs  already exists\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "Derivative type | Calc norm  | FD norm    | Abs error norm | Rel error norm \n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "Gradient        | 1.5274e-01 | 1.5274e-01 | 7.6367e-07     | 4.9999e-06    \n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " \t ===============================\n",
      "\t ModOpt final iteration summary:\n",
      "\t ===============================\n",
      "\t Problem       : x^4\n",
      "\t Solver        : steepest_descent\n",
      "\t itr           : 100\n",
      "\t obj           : 2.830456199167557e-06\n",
      "\t opt           : 0.00023211051832047348\n",
      "\t time          : 0.10356998443603516\n",
      "\t =====================================\n",
      "\n",
      "\n",
      "====================================================================\n",
      "                       modOpt summary table:                        \n",
      "====================================================================\n",
      "             itr              obj              opt             time \n",
      "               0     1.620000E-02     1.527351E-01     4.887581E-05 \n",
      "               1     2.717909E-03     4.003858E-02     1.588106E-03 \n",
      "               2     1.435827E-03     2.481013E-02     3.699064E-03 \n",
      "               3     9.123600E-04     1.765742E-02     5.273819E-03 \n",
      "               4     6.383038E-04     1.350744E-02     7.170200E-03 \n",
      "               5     4.744946E-04     1.081374E-02     8.621216E-03 \n",
      "               6     3.679275E-04     8.935609E-03     9.970903E-03 \n",
      "               7     2.943483E-04     7.558728E-03     1.141405E-02 \n",
      "               8     2.412398E-04     6.510874E-03     1.276493E-02 \n",
      "               9     2.015609E-04     5.689934E-03     1.388383E-02 \n",
      "              10     1.710840E-04     5.031639E-03     1.469803E-02 \n",
      "              11     1.471372E-04     4.493599E-03     1.578903E-02 \n",
      "              12     1.279604E-04     4.046775E-03     1.653790E-02 \n",
      "              13     1.123533E-04     3.670640E-03     1.776481E-02 \n",
      "              14     9.947358E-05     3.350297E-03     1.889205E-02 \n",
      "              15     8.871518E-05     3.074687E-03     2.021503E-02 \n",
      "              16     7.963259E-05     2.835441E-03     2.133799E-02 \n",
      "              17     7.189216E-05     2.626114E-03     2.260685E-02 \n",
      "              18     6.523983E-05     2.441671E-03     2.411914E-02 \n",
      "              19     5.947928E-05     2.278121E-03     2.544498E-02 \n",
      "              20     5.445680E-05     2.132267E-03     2.650809E-02 \n",
      "              21     5.005061E-05     2.001518E-03     2.751589E-02 \n",
      "              22     4.616319E-05     1.883755E-03     2.858901E-02 \n",
      "              23     4.271564E-05     1.777226E-03     2.980399E-02 \n",
      "              24     3.964361E-05     1.680476E-03     3.096604E-02 \n",
      "              25     3.689416E-05     1.592285E-03     3.239393E-02 \n",
      "              26     3.442338E-05     1.511620E-03     3.382301E-02 \n",
      "              27     3.219464E-05     1.437607E-03     3.501081E-02 \n",
      "              28     3.017715E-05     1.369497E-03     3.603792E-02 \n",
      "              29     2.834488E-05     1.306648E-03     3.738189E-02 \n",
      "              30     2.667573E-05     1.248504E-03     3.810787E-02 \n",
      "              31     2.515080E-05     1.194583E-03     3.892899E-02 \n",
      "              32     2.375385E-05     1.144466E-03     4.015899E-02 \n",
      "              33     2.247088E-05     1.097786E-03     4.137397E-02 \n",
      "              34     2.128977E-05     1.054219E-03     4.223108E-02 \n",
      "              35     2.019996E-05     1.013480E-03     4.300499E-02 \n",
      "              36     1.919224E-05     9.753190E-04     4.374886E-02 \n",
      "              37     1.825852E-05     9.395107E-04     4.476690E-02 \n",
      "              38     1.739172E-05     9.058564E-04     4.572487E-02 \n",
      "              39     1.658555E-05     8.741779E-04     4.668283E-02 \n",
      "              40     1.583446E-05     8.443159E-04     4.739475E-02 \n",
      "              41     1.513354E-05     8.161272E-04     4.807186E-02 \n",
      "              42     1.447839E-05     7.894827E-04     4.876184E-02 \n",
      "              43     1.386509E-05     7.642661E-04     4.971504E-02 \n",
      "              44     1.329015E-05     7.403720E-04     5.043101E-02 \n",
      "              45     1.275042E-05     7.177050E-04     5.131292E-02 \n",
      "              46     1.224307E-05     6.961781E-04     5.240512E-02 \n",
      "              47     1.176556E-05     6.757124E-04     5.322909E-02 \n",
      "              48     1.131557E-05     6.562357E-04     5.398798E-02 \n",
      "              49     1.089103E-05     6.376822E-04     5.472398E-02 \n",
      "              50     1.049005E-05     6.199915E-04     5.543089E-02 \n",
      "              51     1.011091E-05     6.031083E-04     5.612707E-02 \n",
      "              52     9.752052E-06     5.869816E-04     5.683208E-02 \n",
      "              53     9.412042E-06     5.715647E-04     5.788302E-02 \n",
      "              54     9.089584E-06     5.568144E-04     5.874395E-02 \n",
      "              55     8.783485E-06     5.426910E-04     5.954790E-02 \n",
      "              56     8.492654E-06     5.291576E-04     6.026506E-02 \n",
      "              57     8.216089E-06     5.161802E-04     6.108809E-02 \n",
      "              58     7.952870E-06     5.037272E-04     6.198883E-02 \n",
      "              59     7.702148E-06     4.917693E-04     6.276679E-02 \n",
      "              60     7.463144E-06     4.802793E-04     6.363916E-02 \n",
      "              61     7.235136E-06     4.692319E-04     6.436896E-02 \n",
      "              62     7.017457E-06     4.586034E-04     6.541491E-02 \n",
      "              63     6.809492E-06     4.483720E-04     6.631994E-02 \n",
      "              64     6.610669E-06     4.385171E-04     6.764793E-02 \n",
      "              65     6.420459E-06     4.290195E-04     6.868887E-02 \n",
      "              66     6.238371E-06     4.198613E-04     6.972289E-02 \n",
      "              67     6.063947E-06     4.110257E-04     7.077575E-02 \n",
      "              68     5.896761E-06     4.024970E-04     7.149005E-02 \n",
      "              69     5.736419E-06     3.942604E-04     7.217598E-02 \n",
      "              70     5.582550E-06     3.863020E-04     7.286787E-02 \n",
      "              71     5.434810E-06     3.786089E-04     7.358384E-02 \n",
      "              72     5.292877E-06     3.711687E-04     7.430983E-02 \n",
      "              73     5.156450E-06     3.639700E-04     7.505322E-02 \n",
      "              74     5.025246E-06     3.570019E-04     7.602692E-02 \n",
      "              75     4.899003E-06     3.502541E-04     7.674098E-02 \n",
      "              76     4.777472E-06     3.437171E-04     7.743597E-02 \n",
      "              77     4.660422E-06     3.373816E-04     7.841587E-02 \n",
      "              78     4.547634E-06     3.312391E-04     7.923412E-02 \n",
      "              79     4.438903E-06     3.252814E-04     8.003402E-02 \n",
      "              80     4.334037E-06     3.195008E-04     8.080101E-02 \n",
      "              81     4.232854E-06     3.138900E-04     8.173490E-02 \n",
      "              82     4.135184E-06     3.084421E-04     8.277893E-02 \n",
      "              83     4.040865E-06     3.031505E-04     8.374786E-02 \n",
      "              84     3.949746E-06     2.980090E-04     8.484793E-02 \n",
      "              85     3.861683E-06     2.930117E-04     8.596516E-02 \n",
      "              86     3.776540E-06     2.881529E-04     8.690095E-02 \n",
      "              87     3.694190E-06     2.834274E-04     8.805299E-02 \n",
      "              88     3.614511E-06     2.788301E-04     8.960891E-02 \n",
      "              89     3.537390E-06     2.743562E-04     9.078503E-02 \n",
      "              90     3.462717E-06     2.700009E-04     9.186792E-02 \n",
      "              91     3.390390E-06     2.657601E-04     9.298301E-02 \n",
      "              92     3.320312E-06     2.616295E-04     9.416103E-02 \n",
      "              93     3.252389E-06     2.576050E-04     9.527302E-02 \n",
      "              94     3.186535E-06     2.536831E-04     9.636402E-02 \n",
      "              95     3.122665E-06     2.498599E-04     9.766793E-02 \n",
      "              96     3.060702E-06     2.461321E-04     9.877610E-02 \n",
      "              97     3.000569E-06     2.424964E-04     9.982014E-02 \n",
      "              98     2.942195E-06     2.389495E-04     1.011560E-01 \n",
      "              99     2.885513E-06     2.354885E-04     1.023400E-01 \n",
      "             100     2.830456E-06     2.321105E-04     1.035700E-01 \n",
      "====================================================================\n",
      "\n",
      "\n",
      "100\n",
      "[0.03449107 0.03449107]\n",
      "0.10466408729553223\n",
      "2.830456199167557e-06\n",
      "0.00023211051832047348\n"
     ]
    }
   ],
   "source": [
    "# Set your optimality tolerance\n",
    "opt_tol = 1E-8\n",
    "# Set maximum optimizer iteration limit\n",
    "maxiter = 100\n",
    "\n",
    "prob = X4()\n",
    "\n",
    "# Set up your optimizer with your problem and pass in optimizer parameters\n",
    "# And declare outputs to be stored\n",
    "optimizer = SteepestDescent(prob,\n",
    "                            opt_tol=opt_tol,\n",
    "                            maxiter=maxiter,\n",
    "                            outputs=['itr', 'obj', 'x', 'opt', 'time'])\n",
    "\n",
    "# Check first derivatives at the initial guess, if needed\n",
    "optimizer.check_first_derivatives(prob.x0)\n",
    "\n",
    "# Solve your optimization problem\n",
    "optimizer.solve()\n",
    "\n",
    "# Print results of optimization (summary_table contains information from each iteration)\n",
    "optimizer.print_results(summary_table=True)\n",
    "\n",
    "# Print any output that was declared\n",
    "# Since the arrays are long, here we only print the last entry and\n",
    "# verify it with the print_results() above\n",
    "\n",
    "print('\\n')\n",
    "print(optimizer.results['itr'])\n",
    "print(optimizer.results['x'])\n",
    "print(optimizer.results['time'])\n",
    "print(optimizer.results['obj'])\n",
    "print(optimizer.results['opt'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
